---
title: "DA5020.P3.Group7"
author: "Thomas Faria, Sairah Shir, Caitlin Kirkpatrick"
date: "2024-08-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load required packages:

```{r, echo = FALSE}
library(tidyverse)
library(lubridate)
library(readr)
library(outliers)
library(psych)
library(fastDummies)
library(patchwork)
library(class) # support package for FNN (returns a class object)
library(FNN) # k-nn classification package
```

## Q1

### Load & Explore Data

```{r loading data}
pre_taxi_data <- read_csv("2018_Green_Taxi_Trip_Data-1.csv")
problems(pre_taxi_data)

# This .csv file requires pre-processing to fix thousands separator commas that are conflicting with .csv format

# There are only a few instances of this error, a text editor was used to fix (remove) the commas in these values

taxi_data <- read_csv("2018_Green_Taxi_Trip_Data-1_fixed.csv")

# Verify that comma errors are handled
problems(taxi_data)
```

```{r initial look}
# Re-classify some variables and handle NA values
taxi_data <- taxi_data %>%
  mutate(
    VendorID = as.factor(VendorID),
    lpep_pickup_datetime = mdy_hm(lpep_pickup_datetime),
    lpep_dropoff_datetime = mdy_hm(lpep_dropoff_datetime),
    RatecodeID = as.factor(RatecodeID),
    payment_type = as.factor(payment_type),
    trip_type = as.factor(trip_type),
    passenger_count = as.factor(passenger_count),
    PULocationID = as.numeric(factor(PULocationID)),
    DOLocationID = as.numeric(factor(DOLocationID)),
    store_and_fwd_flag = as.factor(store_and_fwd_flag)
  ) %>%
  filter(year(lpep_pickup_datetime) == 2018 & year(lpep_dropoff_datetime) == 2018) %>%
  select(-ehail_fee) %>%
  na.omit()

# Check initial summary statistics
summary(taxi_data)
```

* The ehail_fee column is excluded due to only containing NA values
* Some variables that were originally classified as numeric are re-assigned as character types due to being categorical in nature
* Prior to completing any analyses, we can identify erroneous values present in the dataset by examining the minimum and maximum values for some numerical variables:
  * trip_distance has a maximum value of 140.6 miles, which is likely an outlier given
  the long travel time
  * fare_amount has a minimum value of -$183, which is impossible considering this is
  supposed to represent the cost of a trip
  * extra has a minimum value of -4.5, which is also impossible given the minimum
  should be $0.50
  * mta_tax has a minimum value of -0.5, which is incorrect given this variable should
  only represent $0.50 tax charges
  * improvement_surcharge has a minimum value of -0.3, another impossible value as
  there should only be $0.30 surcharges
  * tip_amount has a minimum value of -2.72, which is not possible
  * total_amount has a minimum value of -$183, which is incorrect

```{r outliers}
# Verify categorical observations match what is depicted in the data dictionary
unique(taxi_data$VendorID)
unique(taxi_data$RatecodeID)
unique(taxi_data$store_and_fwd_flag)
unique(taxi_data$payment_type)
unique(taxi_data$trip_type)

# Define function to filter calculated outliers
filter_outliers <- function(df, var) {
  df %>%
    mutate(
      mean_value = mean(df[[var]]),
      sd_value = sd(df[[var]])
    ) %>%
    filter(
      df[[var]] < mean_value - 1.5 * sd_value | 
      df[[var]] > mean_value + 1.5 * sd_value
    ) %>%
    select(all_of(var))
}

# Apply function
trip_distance_outliers <- filter_outliers(taxi_data, "trip_distance")
fare_amount_outliers <- filter_outliers(taxi_data, "fare_amount")
tip_amount_outliers <- filter_outliers(taxi_data, "tip_amount")
tolls_amount_outliers <- filter_outliers(taxi_data, "tolls_amount")
total_amount_outliers <- filter_outliers(taxi_data, "total_amount")

# Outliers
trip_distance_outliers
fare_amount_outliers
tip_amount_outliers
tolls_amount_outliers
total_amount_outliers

# Define function to remove outliers
rmv_outliers <- function(df, var) {
  df %>%
    mutate(
      mean_value = mean(df[[var]]),
      sd_value = sd(df[[var]])
    ) %>%
    filter(
      !(df[[var]] < mean_value - 1.5 * sd_value | 
      df[[var]] > mean_value + 1.5 * sd_value)
    )
}

# Cleaned df
taxi_clean <- taxi_data %>%
  # Remove outliers
  rmv_outliers("trip_distance") %>%
  rmv_outliers("fare_amount") %>%
  rmv_outliers("tip_amount") %>%
  rmv_outliers("tolls_amount") %>%
  rmv_outliers("total_amount") %>%
  # Remove negative values from appropriate columns
  filter(extra >= 0,
         mta_tax >= 0,
         improvement_surcharge >= 0,
         fare_amount >= 0,
         tip_amount >= 0,
         total_amount >= 0) %>%
  # Discard previous stat columns
  select(-mean_value, -sd_value)

taxi_clean
```

* Outliers will be removed as they are likely not representative of typical behavior from passengers, particulary regarding tipping

```{r distributions}
# Add weekday breakdown to clean_taxi for visuals
taxi_clean <- taxi_clean %>%
  mutate(
    pickup_day = as.factor(wday(lpep_pickup_datetime)),
    dropoff_day = as.factor(wday(lpep_dropoff_datetime)),
    mta_tax = as.factor(mta_tax),
    extra = as.factor(extra),
    improvement_surcharge = as.factor(improvement_surcharge)
  ) 
# Histograms for numerical variables
hist_trip <- ggplot(taxi_clean, aes(x = trip_distance)) + geom_histogram()
hist_fare <- ggplot(taxi_clean, aes(x = fare_amount)) + geom_histogram()
hist_tip <- ggplot(taxi_clean, aes(x = tip_amount)) + geom_histogram()
hist_total <- ggplot(taxi_clean, aes(x = total_amount)) + geom_histogram()

  (hist_trip | hist_fare) /
  (hist_tip | hist_total)

# Bar graphs for character variables
bar_pickup_d <- ggplot(taxi_clean, aes(x = pickup_day)) + geom_bar()
bar_dropoff_d <- ggplot(taxi_clean, aes(x = dropoff_day)) + geom_bar()
bar_flag <- ggplot(taxi_clean, aes(x = store_and_fwd_flag)) + geom_bar() 
bar_rate <- ggplot(taxi_clean, aes(x = RatecodeID)) + geom_bar() 
bar_payment <- ggplot(taxi_clean, aes(x = payment_type)) + geom_bar()
bar_trip <- ggplot(taxi_clean, aes(x = trip_type)) + geom_bar()
bar_pass <- ggplot(taxi_clean, aes(x = passenger_count)) + geom_bar()
bar_mta <- ggplot(taxi_clean, aes(x = mta_tax)) + geom_bar()
bar_extra <- ggplot(taxi_clean, aes(x = extra)) + geom_bar()
bar_imp <- ggplot(taxi_clean, aes(x = improvement_surcharge)) + geom_bar()

bar_pu <- ggplot(taxi_clean, aes(x = PULocationID)) + geom_bar()
bar_do <- ggplot(taxi_clean, aes(x = DOLocationID)) + geom_bar()

(bar_pickup_d | bar_dropoff_d | bar_pass) /
  (bar_flag | bar_rate | bar_payment | bar_trip) /
  (bar_imp | bar_mta | bar_extra)
```

* <describe visual trends>

```{r correlations, INCOMPLETE}
# Load in official borough data corresponding to location IDs from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
borough_df <- read.csv("taxi_zone_lookup.csv")

taxi_clean <- taxi_clean %>%
  left_join(borough_df, by = c("PULocationID" = "LocationID")) %>%
  rename(PU_Borough = Borough) %>%
  left_join(borough_df, by = c("DOLocationID" = "LocationID")) %>%
  rename(DO_Borough = Borough)

taxi_clean <- taxi_clean %>%
  mutate(
    PU_Borough = as.factor(PU_Borough),
    DO_Borough = as.factor(DO_Borough)
  )

# Check the structure of the resulting dataframe
str(taxi_clean)

```

```{r}
# Prepare dummy columns
taxi_dummy <- dummy_cols(taxi_clean, select_columns = c("pickup_day", "dropoff_day", "passenger_count", "store_and_fwd_flag", 
                                                        "RatecodeID", "payment_type", "trip_type", "VendorID", "improvement_surcharge",
                                                        "mta_tax", "extra", "PU_Borough", "DO_Borough"), remove_first_dummy = TRUE)

taxi_num <- taxi_dummy %>%
  # Omit original columns
  select(-pickup_day, -dropoff_day, -passenger_count, -store_and_fwd_flag, -VendorID,
         -RatecodeID, -payment_type, -trip_type, -lpep_pickup_datetime, -lpep_dropoff_datetime,
         -mta_tax, -extra, -PULocationID, -DOLocationID, -improvement_surcharge, -PU_Borough, -DO_Borough)

str(taxi_num)
```

```{r}
# Get correlation matrix
taxi_cor <- cor(taxi_num, use = "complete.obs")

# Can exclude later
taxi_cor
```

```{r}
# Convert correlation matrix to df and omit NA values
taxi_cor_df <- as.data.frame(as.table(taxi_cor)) %>%
  na.omit()

# Filter for strong correlations
significant_cor <- taxi_cor_df %>%
  filter((abs(Freq) > 0.7) & (abs(Freq) < 1) & Var1 != Var2)

significant_cor
```

### Feature identification

```{r}

```

### (OPTIONAL) Feature engineering

```{r}

```

## Q2

### Encode the data

```{r}

```

```{r}


```

```{r}

```

### Preprocess the data

```{r}

```

### Normalization

```{r}
# z-score for continuous variables 

continuous_vars <- c('fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount','improvement_surcharge', 'total_amount', 'trip_distance')
continuous_vars

taxi_data[continuous_vars] <- scale(taxi_data[continuous_vars])

# Display summary of the standardized data
summary(taxi_data[continuous_vars])

```

### Creation of training and test data sets

```{r}

# Use sample() to randomly subset
# Setting seed is required for reproducibility

set.seed(1, sample.kind = "Rejection")

index <- sample(c(1, 2),
                size = nrow(taxi_clean),
                prob = c(0.75, 0.25),
                replace = T
                )

taxi_train <- taxi_clean[index==1,]
head(taxi_train)

taxi_test <- taxi_clean[index==2,]
head(taxi_test)

```

-   A 75/25 % split for training/test data was used in this analysis.

    -   The 75/25 split is a pragmatic starting point for exploratory machine learning analysis; 75% training data is generally thought to be large enough to generalize the data patterns of a given set, while 25% test data is typically a good portion for evaluating performance - not so small as to not be representative, while leaving a large enough share of data for training.

    -   Different distributions of training/test may produce more accurate models, an optimization that can be performed after observing initial performance.

## Q3

### Build knn-predict() function

```{r}

# Define the knn_predict() function
# data_train and data_test should be training and test data sets supplied as df or tibble objects
# regress1, regress2 can be any continuous variables chosen for k-nn modeling (input as strings)
# Default response is tip_amount (can input as string value of another response)

knn_predict <- function (data_train, data_test, 
                         regress1, regress2, 
                         k,
                         response = "tip_amount"
                         ) {
  
  # k-nn regression with tip_amount as the default response variable
  knn_reg <- knn.reg(select(data_train, 
                            regress1,
                            regress2),
                     select(data_test, 
                            regress1,
                            regress2),
                     y = data_train[[response]], 
                     k = k
                     )
  
  # Compute MSE
  mse = mean((data_test$tip_amount - knn_reg$pred)^2)
  
  # Return MSE
  return(mse)
  
}

```

## Q4

### Obtain MSE for each k value

```{r}

regress1 <- "trip_distance"
regress2 <- "total_amt"

loop_output <- tibble(k = numeric(), mse = double())

for (i in 1:25) {
  
  mse <- knn_predict(taxi_train, taxi_test,
                     regress1, regress2, i
                     )
  
  loop_output <- add_row(loop_output,
                         k = i, mse = mse)
  
}

print(loop_output)

```

### Visualize k vs. MSE

```{r}

ggplot(loop_output, aes(x = k, y = mse)) +
  geom_line() +
  geom_point(color = "blue") +
  labs(title = "k value vs. MSE",
       x = "k value",
       y = "MSE") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, 
                              face = "bold", 
                              size = 14),
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_line(color = "grey80"), 
    panel.grid.minor = element_line(color = "grey90") 
  )
  
```

### Determine optimal k value

```{r}

# Optimal k value is the k at which MSE is lowest
print(loop_output[which.min(loop_output$mse),])

```

### Discussion

## Q5

### (OPTIONAL) Visual story telling of a dataset attribute

OR

### (OPTIONAL) Optimize k-nn with training/test % split adjustments
