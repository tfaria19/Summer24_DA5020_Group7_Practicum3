---
title: "DA5020.P3.Group7"
author: "Thomas Faria, Sairah Shir, Caitlin Kirkpatrick"
date: "2024-08-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load required packages:

```{r, echo = FALSE}
library(tidyverse)
library(lubridate)
library(readr)
library(outliers)
library(psych)
library(fastDummies)
library(patchwork)
library(class) # support package for FNN (returns a class object)
library(FNN) # k-nn classification package
```

## Q1

### Load & Explore Data

```{r loading data}
pre_taxi_data <- read_csv("2018_Green_Taxi_Trip_Data-1.csv")
problems(pre_taxi_data)

# This .csv file requires pre-processing to fix thousands separator commas that are conflicting with .csv format

# There are only a few instances of this error, a text editor was used to fix (remove) the commas in these values

taxi_data <- read_csv("2018_Green_Taxi_Trip_Data-1_fixed.csv")

# Verify that comma errors are handled
problems(taxi_data)
```

```{r initial look}
# Re-classify some variables and handle NA values
taxi_data <- taxi_data %>%
  mutate(
    VendorID = as.numeric(factor(VendorID)),
    lpep_pickup_datetime = mdy_hm(lpep_pickup_datetime),
    lpep_dropoff_datetime = mdy_hm(lpep_dropoff_datetime),
    RatecodeID = as.numeric(factor(RatecodeID)),
    payment_type = as.numeric(factor(payment_type)),
    trip_type = as.numeric(factor(trip_type)),
    passenger_count = as.numeric(factor(passenger_count)),
    PULocationID = as.numeric(factor(PULocationID)),
    DOLocationID = as.numeric(factor(DOLocationID)),
    store_and_fwd_flag = as.numeric(factor(store_and_fwd_flag))
  ) %>%
  filter(year(lpep_pickup_datetime) == 2018 & year(lpep_dropoff_datetime) == 2018) %>%
  select(-ehail_fee) %>%
  na.omit()

# Check initial summary statistics
summary(taxi_data)
```

* The ehail_fee column is excluded due to only containing NA values
* Some variables that were originally classified as numeric are re-assigned as character types due to being categorical in nature
* Prior to completing any analyses, we can identify erroneous values present in the dataset by examining the minimum and maximum values for some numerical variables:
  * trip_distance has a maximum value of 140.6 miles, which is likely an outlier given the long travel time
  * fare_amount has a minimum value of -$183, which is impossible considering this is supposed to represent the cost of a trip
  * extra has a minimum value of -4.5, which is also impossible given the minimum
  should be $0.50
  * mta_tax has a minimum value of -0.5, which is incorrect given this variable should only represent $0.50 tax charges
  * improvement_surcharge has a minimum value of -0.3, another impossible value as
  there should only be $0.30 surcharges
  * tip_amount has a minimum value of -2.72, which is not possible
  * total_amount has a minimum value of -$183, which is incorrect

```{r outliers}
# Verify categorical observations match what is depicted in the data dictionary
unique(taxi_data$VendorID)
unique(taxi_data$RatecodeID)
unique(taxi_data$store_and_fwd_flag)
unique(taxi_data$payment_type)
unique(taxi_data$trip_type)

# Define function to filter calculated outliers
filter_outliers <- function(df, var) {
  df %>%
    mutate(
      mean_value = mean(df[[var]]),
      sd_value = sd(df[[var]])
    ) %>%
    filter(
      df[[var]] < mean_value - 1.5 * sd_value | 
      df[[var]] > mean_value + 1.5 * sd_value
    ) %>%
    select(all_of(var))
}

# Apply function
trip_distance_outliers <- filter_outliers(taxi_data, "trip_distance")
fare_amount_outliers <- filter_outliers(taxi_data, "fare_amount")
tip_amount_outliers <- filter_outliers(taxi_data, "tip_amount")
tolls_amount_outliers <- filter_outliers(taxi_data, "tolls_amount")
total_amount_outliers <- filter_outliers(taxi_data, "total_amount")

# Outliers
trip_distance_outliers
fare_amount_outliers
tip_amount_outliers
tolls_amount_outliers
total_amount_outliers

# Define function to remove outliers
rmv_outliers <- function(df, var) {
  df %>%
    mutate(
      mean_value = mean(df[[var]]),
      sd_value = sd(df[[var]])
    ) %>%
    filter(
      !(df[[var]] < mean_value - 1.5 * sd_value | 
      df[[var]] > mean_value + 1.5 * sd_value)
    )
}

# Cleaned df
taxi_clean <- taxi_data %>%
  # Remove outliers
  rmv_outliers("trip_distance") %>%
  rmv_outliers("fare_amount") %>%
  rmv_outliers("tip_amount") %>%
  rmv_outliers("tolls_amount") %>%
  rmv_outliers("total_amount") %>%
  # Remove negative values from appropriate columns
  filter(extra >= 0,
         mta_tax >= 0,
         improvement_surcharge >= 0,
         fare_amount >= 0,
         tip_amount >= 0,
         total_amount >= 0) %>%
  # Discard previous stat columns
  select(-mean_value, -sd_value)

taxi_clean
```

* Outliers will be removed as they are likely not representative of typical behavior from passengers, particularly regarding tipping

```{r boroughs}
# Load in official borough data corresponding to location IDs from https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
borough_df <- read.csv("taxi_zone_lookup.csv")
borough_df

taxi_clean <- taxi_clean %>%
  left_join(borough_df, by = c("PULocationID" = "LocationID")) %>%
  rename(PU_Borough = Borough) %>%
  left_join(borough_df, by = c("DOLocationID" = "LocationID")) %>%
  rename(DO_Borough = Borough)
taxi_clean
```

* Data provided by the NYC official government site associates pickup and dropoff location IDs with one of the seven boroughs in Manhattan
* This borough information may be used to "cluster" the taxi zones and simplify analyses without including 200+ individual pickup and dropoff taxi zones
* If needed, taxi zone(s) can be backtraced from a borough during the downstream analysis

```{r distributions}
# Add weekday breakdown, ordinal encoding for remaining factor variables
taxi_clean <- taxi_clean %>%
  mutate(
    pickup_day = as.factor(wday(lpep_pickup_datetime)),
    dropoff_day = as.factor(wday(lpep_dropoff_datetime)),
    mta_tax = as.factor(mta_tax),
    extra = as.factor(extra),
    improvement_surcharge = as.factor(improvement_surcharge),
    PU_Borough = as.factor(PU_Borough),
    DO_Borough = as.factor(DO_Borough),
    tolls_amount = as.factor(tolls_amount)
  )

# Histograms for numerical variables
hist_trip <- ggplot(taxi_clean, aes(x = trip_distance)) + geom_histogram()
hist_fare <- ggplot(taxi_clean, aes(x = fare_amount)) + geom_histogram()
hist_tip <- ggplot(taxi_clean, aes(x = tip_amount)) + geom_histogram()
hist_total <- ggplot(taxi_clean, aes(x = total_amount)) + geom_histogram()

  (hist_trip | hist_fare) /
  (hist_tip | hist_total)

# Bar graphs for character variables
bar_pickup_d <- ggplot(taxi_clean, aes(x = pickup_day)) + geom_bar()
bar_dropoff_d <- ggplot(taxi_clean, aes(x = dropoff_day)) + geom_bar()
bar_flag <- ggplot(taxi_clean, aes(x = store_and_fwd_flag)) + geom_bar() 
bar_rate <- ggplot(taxi_clean, aes(x = RatecodeID)) + geom_bar() 
bar_payment <- ggplot(taxi_clean, aes(x = payment_type)) + geom_bar()
bar_trip <- ggplot(taxi_clean, aes(x = trip_type)) + geom_bar()
bar_pass <- ggplot(taxi_clean, aes(x = passenger_count)) + geom_bar()
bar_mta <- ggplot(taxi_clean, aes(x = mta_tax)) + geom_bar()
bar_extra <- ggplot(taxi_clean, aes(x = extra)) + geom_bar()
bar_imp <- ggplot(taxi_clean, aes(x = improvement_surcharge)) + geom_bar()
bar_pu <- ggplot(taxi_clean, aes(x = PU_Borough)) + geom_bar()
bar_do <- ggplot(taxi_clean, aes(x = DO_Borough)) + geom_bar()
bar_toll <- ggplot(taxi_clean, aes(x = tolls_amount)) + geom_bar()

(bar_dropoff_d | bar_pass | bar_extra | bar_toll) /
  (bar_flag | bar_rate | bar_payment | bar_trip) /
  (bar_imp | bar_mta | bar_pu | bar_do)
```
```{r}
# Final summary check with cleaned df
summary(taxi_clean)
```
* In examining the distributions of the continuous variables:
  * trip_distance has a slight right skew with a median of 1.3 and mean of 1.5
  * tip_amount is significantly skewed toward 0
  * fare_amount has a slight right skew with a median of 7.5 and mean of 8
  * total_amount has a very slight right skew with a median of 9.3 and mean of 9.8

```{r}
# Convert any factors to integers for correlation matrix
taxi_clean <- taxi_clean %>%
  mutate(
    VendorID = as.integer(VendorID),
    store_and_fwd_flag = as.integer(store_and_fwd_flag),
    RatecodeID = as.integer(RatecodeID),
    passenger_count = as.integer(passenger_count),
    extra = as.integer(extra),
    mta_tax = as.integer(mta_tax),
    improvement_surcharge = as.integer(improvement_surcharge),
    payment_type = as.integer(payment_type),
    trip_type = as.integer(trip_type),
    PU_Borough = as.integer(PU_Borough),
    DO_Borough = as.integer(DO_Borough),
    pickup_day = as.integer(pickup_day),
    dropoff_day = as.integer(dropoff_day),
    tolls_amount = as.integer(tolls_amount)
  )

# Get correlation matrix
taxi_cor <- taxi_clean %>%
  select(-lpep_pickup_datetime, -lpep_dropoff_datetime, -PULocationID, -DOLocationID, -pickup_day)
  cor(taxi_cor, use = "complete.obs")

taxi_cor
```

### Feature identification

* When considering features that potentially affect whether or not a tip is given, the following should be considered about tip_amount:
  * Most passengers do not tip as revealed in the visuals and summary statistics
  * When a tip is given, it is under $5
* The following features appeared to have significant (> 0.70 or < -0.7) correlations with tip_amount based on cor():
  * 
* Some features showed little to no correlation but may be kept due to their practical relationship to tip_amount; while cor() may show little relationship, there is no way to completely rule them out:
  * 
* Some features can be eliminated entirely due to being completely independent of the passenger; as in, this information is for internal use only for a driver:
  * store_and_fwd_flag: 
  * VendorID: 

### (OPTIONAL) Feature engineering

```{r}

```

## Q2

### Encode the data

```{r}

```

<<<<<<< HEAD

### Preprocess the data 

```{r}


```

```{r}

```

### Preprocess the data

```{r}

```

### Normalization

```{r}
# min-max normalization for continuous variables 

continuous_vars <- c('fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount','improvement_surcharge', 'total_amount', 'trip_distance')
continuous_vars

min_max_normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Apply min-max normalization to the continuous variables
taxi_clean[continuous_vars] <- lapply(taxi_clean[continuous_vars], min_max_normalize)

# Display summary of the normalized data
summary(taxi_clean[continuous_vars])

```
* The min-max brings the values of a feature within a specific range, from between 0 to 1 
* Using min-max normalization will make sure we have the exact same scale 
* Min-max normalization is sensitive to outliers, however, since we decided to remove the outliers from our dataset we used max-min normalization

### Creation of training and test data sets

```{r}

# Use sample() to randomly subset
# Setting seed is required for reproducibility

set.seed(1, sample.kind = "Rejection")

index <- sample(c(1, 2),
                size = nrow(taxi_clean),
                prob = c(0.75, 0.25),
                replace = T
                )

taxi_train <- taxi_clean[index==1,]
head(taxi_train)

taxi_test <- taxi_clean[index==2,]
head(taxi_test)

```

-   A 75/25 % split for training/test data was used in this analysis.

    -   The 75/25 split is a pragmatic starting point for exploratory machine learning analysis; 75% training data is generally thought to be large enough to generalize the data patterns of a given set, while 25% test data is typically a good portion for evaluating performance - not so small as to not be representative, while leaving a large enough share of data for training.

    -   Different distributions of training/test may produce more accurate models, an optimization that can be performed after observing initial performance.

## Q3

### Build knn-predict() function

```{r}

# Define the knn_predict() function
# data_train and data_test should be training and test data sets supplied as df or tibble objects
# regress1, regress2 can be any continuous variables chosen for k-nn modeling (input as strings)
# Default response is tip_amount (can input as string value of another response)

knn_predict <- function (data_train, data_test, 
                         regress1, regress2, 
                         k,
                         response = "tip_amount"
                         ) {
  
  # k-nn regression with tip_amount as the default response variable
  knn_reg <- knn.reg(select(data_train, 
                            regress1,
                            regress2),
                     select(data_test, 
                            regress1,
                            regress2),
                     y = data_train[[response]], 
                     k = k
                     )
  
  # Compute MSE
  mse = mean((data_test$tip_amount - knn_reg$pred)^2)
  
  # Return MSE
  return(mse)
  
}

```

## Q4

### Obtain MSE for each k value

```{r}

regress1 <- "trip_distance"
regress2 <- "total_amt"

loop_output <- tibble(k = numeric(), mse = double())

for (i in 1:25) {
  
  mse <- knn_predict(taxi_train, taxi_test,
                     regress1, regress2, i
                     )
  
  loop_output <- add_row(loop_output,
                         k = i, mse = mse)
  
}

print(loop_output)

```

### Visualize k vs. MSE

```{r}

ggplot(loop_output, aes(x = k, y = mse)) +
  geom_line() +
  geom_point(color = "blue") +
  labs(title = "k value vs. MSE",
       x = "k value",
       y = "MSE") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, 
                              face = "bold", 
                              size = 14),
    axis.title.x = element_text(size = 12), 
    axis.title.y = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_line(color = "grey80"), 
    panel.grid.minor = element_line(color = "grey90") 
  )
  
```

### Determine optimal k value

```{r}

# Optimal k value is the k at which MSE is lowest
print(loop_output[which.min(loop_output$mse),])

```

### Discussion

## Q5

### (OPTIONAL) Visual story telling of a dataset attribute

* the x axis represents the tip amount and the y axis represents the density, meaning how often a tip amount occurs 
* the higher peaks means that tip amount was given more frequently 
* the area under the curve highlights that the tip amount were more common in that range 
* facet wrap so the different boroughs could be shown and compared 
* from the graphs we find that across all boroughs for pickup and drop off that tipping was not done seeing as the peaks in all the graphs are up at zero. 
* 

```{r}

taxi_clean$PU_Borough <- factor(taxi_clean$PU_Borough, 
                                 levels = c("1", "2", "3", "4", "5"),
                                 labels = c("EWR", "Queens", "Bronx", "Manhattan", "Staten Island"))

# Density plot faceted by pickup borough
ggplot(taxi_clean, aes(x = tip_amount, fill = PU_Borough)) +
  geom_density(alpha = 0.5) +
  labs(title = "Tip Amount Distribution by Pickup Borough", x = "Tip Amount ($)", y = "Density") +
  theme_minimal() +
  facet_wrap(~ PU_Borough, scales = "fixed")

taxi_clean$DO_Borough <- factor(taxi_clean$DO_Borough, 
                                 levels = c("1", "2", "3", "4", "5"),
                                 labels = c("EWR", "Queens", "Bronx", "Manhattan", "Staten Island"))

# Density plot faceted by dropoff borough
ggplot(taxi_clean, aes(x = tip_amount, fill = DO_Borough)) +
  geom_density(alpha = 0.5) +
  labs(title = "Tip Amount Distribution by Drop off Borough", x = "Tip Amount ($)", y = "Density") +
  theme_minimal() +
  facet_wrap(~ DO_Borough, scales = "fixed")

```

OR

### (OPTIONAL) Optimize k-nn with training/test % split adjustments
